{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering\n",
    "\n",
    "This notebook demonstrates a relatively simple approach to automated record clustering based on keywords contents.\n",
    "\n",
    "The purpose of the notebook is to:\n",
    " * demonstrate use of record management APIs\n",
    " * show that it is practical to use machine learning algorithms to automatically discover document clusters\n",
    " * provide refernce implementation that could be enhanced\n",
    " * generate a model that can subsequently be used to demonstration of automatic record classification\n",
    "\n",
    "In a production system, unsupervised clustering techniques such as this would probably be used in an ensemble with ontology-based classification as well as supervised learning of labels that have been applied by human record mangers to a reference set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the packages to be used\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching data from the API\n",
    "\n",
    "The first step in this recipie is to fetch data from the search API (due to a small current bug, we are actually using the raw elasticsearch API behind the Platform Search API.\n",
    "\n",
    "This code is wrapped in a class with two methods -- `get_total()` and `get_records()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "class ESInterface(object):\n",
    "    \"\"\" Interface to ElasticSearch instance \"\"\"\n",
    "    \n",
    "    def __init__(self, url, username, password):\n",
    "        \"\"\" Initializes object, creates ES connection \"\"\"\n",
    "        # Create the ES connection\n",
    "        self.conn = Elasticsearch(url, verify_certs=True, http_auth=(username, password))\n",
    "        self.index_name = 'digitalrecords-search'\n",
    "        self.doc_type = 'modelresult'\n",
    "        # Query to get records that have keywords\n",
    "        self.query = {\n",
    "           'query': {\n",
    "              'exists': {\n",
    "                 'field': 'keywords'\n",
    "              }\n",
    "           },\n",
    "           '_source': ['id', 'keywords'],\n",
    "           'size': 1000\n",
    "        }\n",
    "        # Get the total number of matching records\n",
    "        self.get_total()\n",
    "        \n",
    "    def get_total(self):\n",
    "        \"\"\" Gets the total number of records that match the query \"\"\"\n",
    "        query = self.query\n",
    "        query['size'] = 1\n",
    "        search = self.conn.search(body=query, index=self.index_name)\n",
    "        self.total = search['hits']['total']\n",
    "        \n",
    "    def get_records(self, test_run=False):\n",
    "        print('Total matches: %s' % self.total)\n",
    "        results = []\n",
    "        records = scan(self.conn, index=self.index_name, query=self.query, doc_type=self.doc_type)\n",
    "        if test_run:\n",
    "            records_num = 50000\n",
    "        else:\n",
    "            records_num = self.total\n",
    "        if test_run:\n",
    "            print('Total records to fetch: %s (TEST RUN)' % records_num)\n",
    "        else:\n",
    "            print('Total records to fetch: %s' % records_num)\n",
    "        print('\\nNow fetching records from ES...')\n",
    "        for counter, record in enumerate(records, 1):\n",
    "            if counter > records_num:\n",
    "                break\n",
    "            else:\n",
    "                results += [record['_source']]\n",
    "            if counter % 10000 == 0:\n",
    "                print(' - Fetched %s records out of %s' % (counter, records_num))\n",
    "        print('Finished fetching records from ES\\n')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class can now be used to access metadata from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches: 16899580\n",
      "Total records to fetch: 50000 (TEST RUN)\n",
      "\n",
      "Now fetching records from ES...\n",
      " - Fetched 10000 records out of 50000\n",
      " - Fetched 20000 records out of 50000\n",
      " - Fetched 30000 records out of 50000\n",
      " - Fetched 40000 records out of 50000\n",
      " - Fetched 50000 records out of 50000\n",
      "Finished fetching records from ES\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enter the ES credentials\n",
    "ES_url = 'https://b8cecd38087b50c322e84733eb56a855.ap-southeast-2.aws.found.io:9243/'\n",
    "ES_username = 'elastic'\n",
    "ES_password = 'EwbDDTN54MnWni7E6Du0l8hG'\n",
    "\n",
    "# Use the credentials to instantiate an ESInterface instance\n",
    "elasticsearch = ESInterface(ES_url, ES_username, ES_password)\n",
    "\n",
    "# Fetch the records in test_run mode, which means it will only get 50,000 matching records\n",
    "records = elasticsearch.get_records(test_run=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the data needs to be cleaned up a little bit before it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>records.recordname.ab8b1dce-d563-4e5d-b9ba-a9f...</td>\n",
       "      <td>Faculty, Chapters, Contact, Location, Conferen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>records.recordname.ba200f45-6c0d-479c-9f25-d60...</td>\n",
       "      <td>patience, Comments, Archives, courage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>records.recordname.dbc97c7a-e526-4e5f-a1f5-8fc...</td>\n",
       "      <td>Attributes, Documents, filters, Resources, Mic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>records.recordname.8d1f4d87-3cd4-4bf0-a129-be0...</td>\n",
       "      <td>Otwarcia, Centrum, Mochnackiego, Obsługi, Cent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>records.recordname.a3377b20-c7e7-4d7c-bb44-f0f...</td>\n",
       "      <td>Centrum, Informacje, Obsługi, Aktualności, Cen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  records.recordname.ab8b1dce-d563-4e5d-b9ba-a9f...   \n",
       "1  records.recordname.ba200f45-6c0d-479c-9f25-d60...   \n",
       "2  records.recordname.dbc97c7a-e526-4e5f-a1f5-8fc...   \n",
       "3  records.recordname.8d1f4d87-3cd4-4bf0-a129-be0...   \n",
       "4  records.recordname.a3377b20-c7e7-4d7c-bb44-f0f...   \n",
       "\n",
       "                                            keywords  \n",
       "0  Faculty, Chapters, Contact, Location, Conferen...  \n",
       "1              patience, Comments, Archives, courage  \n",
       "2  Attributes, Documents, filters, Resources, Mic...  \n",
       "3  Otwarcia, Centrum, Mochnackiego, Obsługi, Cent...  \n",
       "4  Centrum, Informacje, Obsługi, Aktualności, Cen...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a dataframe out of the records\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Replace NaN's with empty string\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Make sure the keywords column is of string data type\n",
    "df['keywords'] = df['keywords'].astype(str)\n",
    "\n",
    "# Show a sample of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowv we need to create a tokenize method to extract individual words (remove punctuation, stopwords, etc).\n",
    "\n",
    "The `tokenize()` function that we create below is used by built-in vectorization classes from the SciKit-Learn library, as well as stopwords and punctuation libraries from the venerable NLTK (the Naural Language Took Kit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # get the english stopwords\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.wordpunct_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        # include only those that contains letters\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            # exclude stop words, those shorter than 3 characters, and those that\n",
    "            # start with non-alphanumeric characters\n",
    "            if token not in stopwords and len(token) > 2 and token[0].isalnum():\n",
    "                filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "We will use a vectorizer called TF-IDF (Term-Frequency times Inverse Document-Frequency).\n",
    "\n",
    "The tokenization method above converts extracted text into a feature array. We need this to feed for our algorithm that determines salient features by comparing the frequencies of these features across different documents.\n",
    "\n",
    "Read more here: http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=200000,\n",
    "    stop_words='english',\n",
    "    use_idf=True,\n",
    "    tokenizer=tokenize)\n",
    "\n",
    "feature_matrix = vectorizer.fit_transform(df['keywords'])\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "For a simple demonstration, we use one of the fastest clustering algorithms available -- namely, k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>records.recordname.ab8b1dce-d563-4e5d-b9ba-a9f...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>records.recordname.ba200f45-6c0d-479c-9f25-d60...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>records.recordname.dbc97c7a-e526-4e5f-a1f5-8fc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>records.recordname.8d1f4d87-3cd4-4bf0-a129-be0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>records.recordname.a3377b20-c7e7-4d7c-bb44-f0f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  cluster\n",
       "0  records.recordname.ab8b1dce-d563-4e5d-b9ba-a9f...        7\n",
       "1  records.recordname.ba200f45-6c0d-479c-9f25-d60...        8\n",
       "2  records.recordname.dbc97c7a-e526-4e5f-a1f5-8fc...        0\n",
       "3  records.recordname.8d1f4d87-3cd4-4bf0-a129-be0...        0\n",
       "4  records.recordname.a3377b20-c7e7-4d7c-bb44-f0f...        0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the k-means clustering\n",
    "nclusters = 10\n",
    "km = KMeans(n_clusters=nclusters)\n",
    "km.fit(feature_matrix)\n",
    "\n",
    "# Create a dataframe out of the formed clusters\n",
    "clusters = km.labels_.tolist()\n",
    "docs = { 'id': df['id'], 'cluster': clusters }\n",
    "clusters_df = pd.DataFrame(docs, columns = ['id', 'cluster'])\n",
    "\n",
    "# Show a sample of the result of the clustering\n",
    "clusters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the keywords/terms per cluster, we need to extract the k-means clustering object from the indices of terms that compose each cluster. In order to get the top defining terms, we reverse the ordering so we get the closest term to the cluster center as the first element in the list. These steps are accomplished in a single of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         count                                           keywords\n",
      "cluster                                                          \n",
      "0        39207  resources, related, services, information, con...\n",
      "1          871  request, password, information, completed, con...\n",
      "2          296  twitter, welcome, chapter, followers, membersh...\n",
      "3          309  resource, looking, temporarily, removed, unava...\n",
      "4         1639  navigation, primary, sidebar, contact, related...\n",
      "5         1222  newsletter, contact, subscribe, categories, si...\n",
      "6         1043  facebook, twitter, popular, connect, subscribe...\n",
      "7         2256  contact, information, kennesaw, marietta, init...\n",
      "8         1946  categories, archives, comments, subscribe, pop...\n",
      "9         1211  service, address, protection, customer, compan...\n"
     ]
    }
   ],
   "source": [
    "# We proceed further to get only the top 20 terms per cluster\n",
    "clusters = range(nclusters)\n",
    "cluster_terms = {k: [] for k in clusters}\n",
    "for i in clusters:\n",
    "    cluster_top_terms = [feature_names[x] for x in order_centroids[i, :20]]\n",
    "    cluster_terms[i] = cluster_top_terms\n",
    "\n",
    "# We fuse the raw dataframe with the clusters dataframe so get\n",
    "# a mapping of the record ID, keywords, and cluster in one dataframe\n",
    "df.set_index('id', inplace=True)\n",
    "clusters_df.set_index('id', inplace=True)\n",
    "combined_df = df.join(clusters_df, how='outer')\n",
    "\n",
    "# Then from the combined dataframe we can generate a good summary\n",
    "# of the clusters with cluster number, the count of records, and their defining terms\n",
    "cdf = combined_df.groupby('cluster').agg('count')\n",
    "cdf.rename(columns={'keywords': 'count'}, inplace=True)\n",
    "cdf['keywords'] = [', '.join(cluster_terms[x]) for x in cdf.index]\n",
    "print(cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have ten clusters defined by a collection of characteristic keywords. The keywords associated with these clusters can be used to demonstrate record auto-classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
