{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering\n",
    "\n",
    "This notebook demonstrates a relatively simple approach to automated record clustering based on keywords contents.\n",
    "\n",
    "The purpose of the notebook is to:\n",
    " * demonstrate use of record management APIs\n",
    " * show that it is practical to use machine learning algorithms to automatically discover document clusters\n",
    " * provide refernce implementation that could be enhanced\n",
    " * generate a model that can subsequently be used to demonstration of automatic record classification\n",
    "\n",
    "In a production system, unsupervised clustering techniques such as this would probably be used in an ensemble with ontology-based classification as well as supervised learning of labels that have been applied by human record mangers to a reference set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all the packages to be used\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching data from the API\n",
    "\n",
    "The first step in this recipie is to fetch data from the search API (due to a small current bug, we are actually using the raw elasticsearch API behind the Platform Search API.\n",
    "\n",
    "This code is wrapped in a class with two main methods -- `get_total()` and `get_records()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class SearchInterface(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = 'https://digitalrecords.showthething.com/api/search/v0/records/'\n",
    "        self.get_total()\n",
    "        \n",
    "    def get_total(self):\n",
    "        \"\"\" Gets the total number of records that match the query \"\"\"\n",
    "        resp = self._fetch_batch(0, limit=1)\n",
    "        data = resp.json()\n",
    "        self.total = data['hits']['count']\n",
    "        \n",
    "    def _fetch_batch(self, offset, limit=1000):\n",
    "        # The `exists`: `keywords` params makes sure that only those records with\n",
    "        # keywords are fetched from the database\n",
    "        params = {'limit': limit, 'offset': offset, 'exists': 'keywords'}\n",
    "        resp = requests.get(self.base_url, params=params)\n",
    "        return resp\n",
    "    \n",
    "    def get_records(self, test_run=False):\n",
    "        print('Total matches: %s' % self.total)\n",
    "        if test_run:\n",
    "            # We just limit to the 10,000 records for the test run\n",
    "            records_num = 10000\n",
    "            print('Total records to fetch: %s (TEST RUN)' % records_num)\n",
    "        else:\n",
    "            records_num = self.total\n",
    "            print('Total records to fetch: %s' % records_num)\n",
    "        results = []\n",
    "        batches = range(0, self.total, 1000)\n",
    "        print('\\nNow fetching records from ES...')\n",
    "        for batch_offset in batches:\n",
    "            fetch_count = batch_offset + 1000\n",
    "            if fetch_count <= records_num:\n",
    "                resp = self._fetch_batch(batch_offset)\n",
    "                if resp.status_code == 200:\n",
    "                    data = resp.json()\n",
    "                    results += data['hits']['results']\n",
    "                else:\n",
    "                    raise Exception('Error fetching records from the server!')\n",
    "                print(' - Fetched %s records out of %s' % (fetch_count, records_num))\n",
    "            else:\n",
    "                break\n",
    "        print('Finished fetching records from ES\\n')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class can now be used to access metadata from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches: 16899585\n",
      "Total records to fetch: 10000 (TEST RUN)\n",
      "\n",
      "Now fetching records from ES...\n",
      " - Fetched 1000 records out of 10000\n",
      " - Fetched 2000 records out of 10000\n",
      " - Fetched 3000 records out of 10000\n",
      " - Fetched 4000 records out of 10000\n",
      " - Fetched 5000 records out of 10000\n",
      " - Fetched 6000 records out of 10000\n",
      " - Fetched 7000 records out of 10000\n",
      " - Fetched 8000 records out of 10000\n",
      " - Fetched 9000 records out of 10000\n",
      " - Fetched 10000 records out of 10000\n",
      "Finished fetching records from ES\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch the records in test_run mode, which means it will only get 10,000 matching records\n",
    "search = SearchInterface()\n",
    "records = search.get_records(test_run=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the data needs to be cleaned up a little bit before it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6306</th>\n",
       "      <td>200660a4-e3e6-469b-914d-fc08afd5a2fc</td>\n",
       "      <td>['Connecting', 'Hailemariam', 'Alliance', 'Adm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>2900d1ae-73b0-4125-a5c5-42bdc09c6d2d</td>\n",
       "      <td>['Turbines', 'County', 'Independent', 'Novembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>4c2f6ae1-446f-4079-b0f5-e2d03cab9d3e</td>\n",
       "      <td>['requested']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551</th>\n",
       "      <td>27e40095-ca0c-41aa-b75a-2697eca7c7c8</td>\n",
       "      <td>['September', 'Similar', 'Coupons', 'Coupon:']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>72877548-5b51-4b42-9e2a-379dad0bbd69</td>\n",
       "      <td>['Related', 'project', 'Response', 'Tonight', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>41c08d8c-e960-49b2-956a-6bc2976e56de</td>\n",
       "      <td>['version', 'target role', 'associations', 'do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>744c268e-652d-4df9-8caa-beb87ae59510</td>\n",
       "      <td>['February', 'Wellbeing', 'Navigation', 'Educa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>1ab20d81-e62c-4d55-b71c-bd0647ecf1df</td>\n",
       "      <td>['modified', 'easy', 'nameclassifier', 'diffic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8972</th>\n",
       "      <td>ae75d92d-2e39-48b6-9157-84d4d4a83246</td>\n",
       "      <td>['detention', 'assessment', 'Harvesting', 'eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3856</th>\n",
       "      <td>368e4a9c-3942-47c3-9552-244df3c78098</td>\n",
       "      <td>['Photographer', 'Photography', 'Beautiful', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        id  \\\n",
       "6306  200660a4-e3e6-469b-914d-fc08afd5a2fc   \n",
       "4495  2900d1ae-73b0-4125-a5c5-42bdc09c6d2d   \n",
       "1986  4c2f6ae1-446f-4079-b0f5-e2d03cab9d3e   \n",
       "3551  27e40095-ca0c-41aa-b75a-2697eca7c7c8   \n",
       "4107  72877548-5b51-4b42-9e2a-379dad0bbd69   \n",
       "1182  41c08d8c-e960-49b2-956a-6bc2976e56de   \n",
       "1416  744c268e-652d-4df9-8caa-beb87ae59510   \n",
       "289   1ab20d81-e62c-4d55-b71c-bd0647ecf1df   \n",
       "8972  ae75d92d-2e39-48b6-9157-84d4d4a83246   \n",
       "3856  368e4a9c-3942-47c3-9552-244df3c78098   \n",
       "\n",
       "                                               keywords  \n",
       "6306  ['Connecting', 'Hailemariam', 'Alliance', 'Adm...  \n",
       "4495  ['Turbines', 'County', 'Independent', 'Novembe...  \n",
       "1986                                      ['requested']  \n",
       "3551     ['September', 'Similar', 'Coupons', 'Coupon:']  \n",
       "4107  ['Related', 'project', 'Response', 'Tonight', ...  \n",
       "1182  ['version', 'target role', 'associations', 'do...  \n",
       "1416  ['February', 'Wellbeing', 'Navigation', 'Educa...  \n",
       "289   ['modified', 'easy', 'nameclassifier', 'diffic...  \n",
       "8972  ['detention', 'assessment', 'Harvesting', 'eff...  \n",
       "3856  ['Photographer', 'Photography', 'Beautiful', '...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build a dataframe out of the records, taking only the uuid and keywords per record\n",
    "df = pd.DataFrame(records, columns=['uuid', 'keywords'])\n",
    "\n",
    "# Let's just rename the `uuid` column to `id`\n",
    "df.rename(columns={'uuid': 'id'}, inplace=True)\n",
    "\n",
    "# Replace NaN's with empty string\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Make sure the keywords column is of string data type\n",
    "df['keywords'] = df['keywords'].astype(str)\n",
    "\n",
    "# Show a sample of the dataframe\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowv we need to create a tokenize method to extract individual words (remove punctuation, stopwords, etc).\n",
    "\n",
    "The `tokenize()` function that we create below is used by built-in vectorization classes from the SciKit-Learn library, as well as stopwords and punctuation libraries from the venerable NLTK (the Naural Language Took Kit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # get the english stopwords\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.wordpunct_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        # include only those that contains letters\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            # exclude stop words, those shorter than 3 characters, and those that\n",
    "            # start with non-alphanumeric characters\n",
    "            if token not in stopwords and len(token) > 2 and token[0].isalnum():\n",
    "                filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "We will use a vectorizer called TF-IDF (Term-Frequency times Inverse Document-Frequency).\n",
    "\n",
    "The tokenization method above converts extracted text into a feature array. We need this to feed for our algorithm that determines salient features by comparing the frequencies of these features across different documents.\n",
    "\n",
    "Read more here: http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Note: We have limited the max_features to 100000 here for faster processing\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=100000,\n",
    "    stop_words='english',\n",
    "    use_idf=True,\n",
    "    tokenizer=tokenize)\n",
    "\n",
    "feature_matrix = vectorizer.fit_transform(df['keywords'])\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "For a simple demonstration, we use one of the fastest clustering algorithms available -- namely, k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>8934834e-5dc1-4a73-8e03-49997418df4e</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3856</th>\n",
       "      <td>368e4a9c-3942-47c3-9552-244df3c78098</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3915</th>\n",
       "      <td>17b4f36a-9773-42a3-a16f-c74867c33552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>b7eeb394-2906-4a27-a008-a8e9db329955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7889</th>\n",
       "      <td>50a854c7-e93c-4f0f-a949-d9daf7b55466</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8512</th>\n",
       "      <td>510cc5d2-db66-443d-8a6d-24566b6d0cce</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>d0f52332-861f-487c-b40d-48891e35cb00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>57dec32d-19a4-4c22-ba86-295a7da35948</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>525023be-1b9e-42fc-afb2-542bb92229a3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5834</th>\n",
       "      <td>9f63d27f-c3d5-4c70-9ae4-95fdd9385409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        id  cluster\n",
       "1332  8934834e-5dc1-4a73-8e03-49997418df4e        1\n",
       "3856  368e4a9c-3942-47c3-9552-244df3c78098        1\n",
       "3915  17b4f36a-9773-42a3-a16f-c74867c33552        1\n",
       "2391  b7eeb394-2906-4a27-a008-a8e9db329955        1\n",
       "7889  50a854c7-e93c-4f0f-a949-d9daf7b55466        1\n",
       "8512  510cc5d2-db66-443d-8a6d-24566b6d0cce        2\n",
       "325   d0f52332-861f-487c-b40d-48891e35cb00        0\n",
       "7100  57dec32d-19a4-4c22-ba86-295a7da35948        1\n",
       "2736  525023be-1b9e-42fc-afb2-542bb92229a3        1\n",
       "5834  9f63d27f-c3d5-4c70-9ae4-95fdd9385409        1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Execute the k-means clustering\n",
    "nclusters = 3 # Split into 3 clusters\n",
    "km = KMeans(n_clusters=nclusters)\n",
    "km.fit(feature_matrix)\n",
    "\n",
    "# Create a dataframe out of the formed clusters\n",
    "clusters = km.labels_.tolist()\n",
    "docs = { 'id': df['id'], 'cluster': clusters }\n",
    "clusters_df = pd.DataFrame(docs, columns = ['id', 'cluster'])\n",
    "\n",
    "# Show a sample of the result of the clustering\n",
    "clusters_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the keywords/terms per cluster, we need to extract the k-means clustering object from the indices of terms that compose each cluster. In order to get the top defining terms, we reverse the ordering so we get the closest term to the cluster center as the first element in the list. These steps are accomplished in a single of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0\n",
      "easy, nameclassifier, udt, author, status, phase, class, proposed, advanced, notes, project, modified, connection, appears, version, type, direction, generalization, complexity, priority \n",
      "\n",
      "Cluster #1\n",
      "related, contact, information, yachting, angling, telephone, categories, navigation, partners, newsletter, description, service, products, upcoming, archives, judonotes, twitter, education, welcome, comment \n",
      "\n",
      "Cluster #2\n",
      "september, coupons, coupon, similar, comments, essendon, warehouse, magazine, watches, business, wholesale, vistaprint, vintage, writers, accessories, furniture, wedding, verizon, wireless, country \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We proceed further to get only the top 20 terms per cluster\n",
    "cluster_terms = {k: [] for k in clusters}\n",
    "for i in range(nclusters):\n",
    "    cluster_top_terms = [feature_names[x] for x in order_centroids[i, :20]]\n",
    "    cluster_terms[i] = cluster_top_terms\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "clusters_df.reset_index(inplace=True)\n",
    "\n",
    "# We fuse the raw dataframe with the clusters dataframe so get\n",
    "# a mapping of the record ID, keywords, and cluster in one dataframe\n",
    "df.set_index('id', inplace=True)\n",
    "clusters_df.set_index('id', inplace=True)\n",
    "combined_df = df.join(clusters_df, how='outer')\n",
    "\n",
    "# Then from the combined dataframe we can generate a good summary\n",
    "# of the clusters with cluster number, the count of records, and their defining terms\n",
    "cdf = combined_df.groupby('cluster').agg('count')\n",
    "cdf.rename(columns={'keywords': 'count'}, inplace=True)\n",
    "cdf['keywords'] = [', '.join(cluster_terms[x]) for x in cdf.index]\n",
    "\n",
    "for i,row in cdf.iterrows():\n",
    "    print('Cluster #%s' % row.name)\n",
    "    print(row.keywords, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have three clusters defined by a collection of characteristic keywords. The keywords associated with these clusters can be used to demonstrate record auto-classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Saving the Clustering Results\n",
    "\n",
    "For the auto-classification step, at least the generated clusters and the vectorizer object are needed. So we need to dumpt those into file. In Python, we do that using the `pickle` package. It serializes any complex Python object/data structure that can then be loaded later on for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "results = {\n",
    "    'vectorizer': vectorizer,\n",
    "    'features': feature_matrix,\n",
    "    'clusters': combined_df,\n",
    "    'terms': cluster_terms\n",
    "}\n",
    "\n",
    "if not os.path.exists('temp'):\n",
    "    os.mkdir('temp')\n",
    "    \n",
    "pickle.dump(results, open('temp/clusters.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}