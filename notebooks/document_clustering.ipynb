{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering\n",
    "\n",
    "This notebook demonstrates a relatively simple approach to automated record clustering based on keywords contents.\n",
    "\n",
    "The purpose of the notebook is to:\n",
    " * demonstrate use of record management APIs\n",
    " * show that it is practical to use machine learning algorithms to automatically discover document clusters\n",
    " * provide refernce implementation that could be enhanced\n",
    " * generate a model that can subsequently be used to demonstration of automatic record classification\n",
    "\n",
    "In a production system, unsupervised clustering techniques such as this would probably be used in an ensemble with ontology-based classification as well as supervised learning of labels that have been applied by human record mangers to a reference set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the packages to be used\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching data from the API\n",
    "\n",
    "The first step in this recipie is to fetch data from the search API (due to a small current bug, we are actually using the raw elasticsearch API behind the Platform Search API.\n",
    "\n",
    "This code is wrapped in a class with two main methods -- `get_total()` and `get_records()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class SearchInterface(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = 'https://digitalrecords.showthething.com/api/search/v0/records/'\n",
    "        self.get_total()\n",
    "        \n",
    "    def get_total(self):\n",
    "        \"\"\" Gets the total number of records that match the query \"\"\"\n",
    "        resp = self._fetch_batch(0, limit=1)\n",
    "        data = resp.json()\n",
    "        self.total = data['hits']['count']\n",
    "        \n",
    "    def _fetch_batch(self, offset, limit=1000):\n",
    "        # The `exists`: `keywords` params makes sure that only those records with\n",
    "        # keywords are fetched from the database\n",
    "        params = {'limit': limit, 'offset': offset, 'exists': 'keywords'}\n",
    "        resp = requests.get(self.base_url, params=params)\n",
    "        return resp\n",
    "    \n",
    "    def get_records(self, test_run=False):\n",
    "        print('Total matches: %s' % self.total)\n",
    "        if test_run:\n",
    "            # We just limit to the 10,000 records for the test run\n",
    "            records_num = 10000\n",
    "            print('Total records to fetch: %s (TEST RUN)' % records_num)\n",
    "        else:\n",
    "            records_num = self.total\n",
    "            print('Total records to fetch: %s' % records_num)\n",
    "        results = []\n",
    "        batches = range(0, self.total, 1000)\n",
    "        print('\\nNow fetching records from ES...')\n",
    "        for batch_offset in batches:\n",
    "            fetch_count = batch_offset + 1000\n",
    "            if fetch_count <= records_num:\n",
    "                resp = self._fetch_batch(batch_offset)\n",
    "                if resp.status_code == 200:\n",
    "                    data = resp.json()\n",
    "                    results += data['hits']['results']\n",
    "                else:\n",
    "                    raise Exception('Error fetching records from the server!')\n",
    "                print(' - Fetched %s records out of %s' % (fetch_count, records_num))\n",
    "            else:\n",
    "                break\n",
    "        print('Finished fetching records from ES\\n')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class can now be used to access metadata from the API. This is currently the slowest part of the process, even with the `test_run=True` parameter limiting it to 50,000 records. The approach of pulling keyword metadata down to a script works for hundreds of thousands of millions of records, but becomes impractical for tens or hundreds of millions of records. Future implementations should use a \"send the code to the data (not the data to the code)\" type of approach, such as map-reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches: 16899588\n",
      "Total records to fetch: 10000 (TEST RUN)\n",
      "\n",
      "Now fetching records from ES...\n",
      " - Fetched 1000 records out of 10000\n",
      " - Fetched 2000 records out of 10000\n",
      " - Fetched 3000 records out of 10000\n",
      " - Fetched 4000 records out of 10000\n",
      " - Fetched 5000 records out of 10000\n",
      " - Fetched 6000 records out of 10000\n",
      " - Fetched 7000 records out of 10000\n",
      " - Fetched 8000 records out of 10000\n",
      " - Fetched 9000 records out of 10000\n",
      " - Fetched 10000 records out of 10000\n",
      "Finished fetching records from ES\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch the records in test_run mode, which means it will only get 10,000 matching records\n",
    "search = SearchInterface()\n",
    "records = search.get_records(test_run=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the data needs to be cleaned up a little bit before it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8466</th>\n",
       "      <td>6ab02346-907b-44a5-be92-ed4da2d08ea0</td>\n",
       "      <td>['Pathway', 'experience', 'Flatwater', 'Traini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4571</th>\n",
       "      <td>f8cca64b-7946-4efc-b60b-88e830af84c1</td>\n",
       "      <td>['Turbines', 'County', 'Independent', 'Natural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553</th>\n",
       "      <td>81911597-1dff-4b26-83c8-ba3dd90e8a5e</td>\n",
       "      <td>['followers', 'Twitter?', 'Twitter']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4696</th>\n",
       "      <td>158cb0ba-a443-4971-b583-d73ddc7c0194</td>\n",
       "      <td>['Lutheran', 'Welcome', 'Upcoming', 'Descripti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4637</th>\n",
       "      <td>44d701a6-d786-4bd7-8f2b-b744e119dd0e</td>\n",
       "      <td>['Independent', 'renewable', 'Elegant', 'Minis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7158</th>\n",
       "      <td>c345f07a-31a2-461d-8611-6fb67908e08e</td>\n",
       "      <td>['Description:', 'Reviews', 'Singlet', 'Newsle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4690</th>\n",
       "      <td>c0bcce72-47a9-432a-a263-eb31004c2575</td>\n",
       "      <td>['Service', 'Lutheran', 'Welcome', 'Illawarra'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13da464a-11f3-472e-8945-d6d1af96c3dc</td>\n",
       "      <td>['u√¢']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6580</th>\n",
       "      <td>31c2c52c-9a43-438e-8c47-275539431f78</td>\n",
       "      <td>['Arboretum', 'euroaarboretum', 'responses']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>c632344a-9a6b-4979-96d4-0e751fe0823f</td>\n",
       "      <td>['19/11/2016', 'ARTICLESMORE', 'COMMING', 'INV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        id  \\\n",
       "8466  6ab02346-907b-44a5-be92-ed4da2d08ea0   \n",
       "4571  f8cca64b-7946-4efc-b60b-88e830af84c1   \n",
       "6553  81911597-1dff-4b26-83c8-ba3dd90e8a5e   \n",
       "4696  158cb0ba-a443-4971-b583-d73ddc7c0194   \n",
       "4637  44d701a6-d786-4bd7-8f2b-b744e119dd0e   \n",
       "7158  c345f07a-31a2-461d-8611-6fb67908e08e   \n",
       "4690  c0bcce72-47a9-432a-a263-eb31004c2575   \n",
       "13    13da464a-11f3-472e-8945-d6d1af96c3dc   \n",
       "6580  31c2c52c-9a43-438e-8c47-275539431f78   \n",
       "1715  c632344a-9a6b-4979-96d4-0e751fe0823f   \n",
       "\n",
       "                                               keywords  \n",
       "8466  ['Pathway', 'experience', 'Flatwater', 'Traini...  \n",
       "4571  ['Turbines', 'County', 'Independent', 'Natural...  \n",
       "6553               ['followers', 'Twitter?', 'Twitter']  \n",
       "4696  ['Lutheran', 'Welcome', 'Upcoming', 'Descripti...  \n",
       "4637  ['Independent', 'renewable', 'Elegant', 'Minis...  \n",
       "7158  ['Description:', 'Reviews', 'Singlet', 'Newsle...  \n",
       "4690  ['Service', 'Lutheran', 'Welcome', 'Illawarra'...  \n",
       "13                                               ['u√¢']  \n",
       "6580       ['Arboretum', 'euroaarboretum', 'responses']  \n",
       "1715  ['19/11/2016', 'ARTICLESMORE', 'COMMING', 'INV...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build a dataframe out of the records, taking only the uuid and keywords per record\n",
    "df = pd.DataFrame(records, columns=['uuid', 'keywords'])\n",
    "\n",
    "# Let's just rename the `uuid` column to `id`\n",
    "df.rename(columns={'uuid': 'id'}, inplace=True)\n",
    "\n",
    "# Replace NaN's with empty string\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Make sure the keywords column is of string data type\n",
    "df['keywords'] = df['keywords'].astype(str)\n",
    "\n",
    "# Show a sample of the dataframe\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowv we need to create a tokenize method to extract individual words (remove punctuation, stopwords, etc).\n",
    "\n",
    "The `tokenize()` function that we create below is used by built-in vectorization classes from the SciKit-Learn library, as well as stopwords and punctuation libraries from the venerable NLTK (the Naural Language Took Kit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # get the english stopwords\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.wordpunct_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        # include only those that contains letters\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            # exclude stop words, those shorter than 3 characters, and those that\n",
    "            # start with non-alphanumeric characters\n",
    "            if token not in stopwords and len(token) > 2 and token[0].isalnum():\n",
    "                filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "We will use a vectorizer called TF-IDF (Term-Frequency times Inverse Document-Frequency).\n",
    "\n",
    "The tokenization method above converts extracted text into a feature array. We need this to feed for our algorithm that determines salient features by comparing the frequencies of these features across different documents.\n",
    "\n",
    "Read more here: http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Note: We have limited the max_features to 100000 here for faster processing\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=100000,\n",
    "    stop_words='english',\n",
    "    use_idf=True,\n",
    "    tokenizer=tokenize)\n",
    "\n",
    "feature_matrix = vectorizer.fit_transform(df['keywords'])\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "For a simple demonstration, we use one of the fastest clustering algorithms available -- namely, k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>08dcf00b-d07f-4763-aa37-d3dbef0fb763</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4077</th>\n",
       "      <td>53615bb3-0f0c-438d-acf7-3a34aa010930</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6168</th>\n",
       "      <td>974c3f38-9347-41a1-b4de-e7279a5fa11c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3134</th>\n",
       "      <td>c94430cc-5e16-44f3-b144-be90a9bd3dd5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>2c0a93ff-4699-4308-a776-fa68fa6a795b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>17b0382b-7fe3-415c-bcd8-2b8d8ff30d70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6752</th>\n",
       "      <td>2c2e0883-3ff3-4c68-9256-86437be21ecd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789</th>\n",
       "      <td>6f0003b0-76d2-48e2-8d37-d8b3e5dd729b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>b3f9bdbd-f262-44aa-a63e-7e23508f3f0b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>26624e55-94e8-4974-ad51-b0f22c7e8244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        id  cluster\n",
       "2885  08dcf00b-d07f-4763-aa37-d3dbef0fb763        0\n",
       "4077  53615bb3-0f0c-438d-acf7-3a34aa010930        0\n",
       "6168  974c3f38-9347-41a1-b4de-e7279a5fa11c        0\n",
       "3134  c94430cc-5e16-44f3-b144-be90a9bd3dd5        0\n",
       "3381  2c0a93ff-4699-4308-a776-fa68fa6a795b        0\n",
       "2898  17b0382b-7fe3-415c-bcd8-2b8d8ff30d70        0\n",
       "6752  2c2e0883-3ff3-4c68-9256-86437be21ecd        0\n",
       "6789  6f0003b0-76d2-48e2-8d37-d8b3e5dd729b        0\n",
       "3375  b3f9bdbd-f262-44aa-a63e-7e23508f3f0b        0\n",
       "5026  26624e55-94e8-4974-ad51-b0f22c7e8244        0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Execute the k-means clustering\n",
    "nclusters = 3 # Split into 3 clusters\n",
    "km = KMeans(n_clusters=nclusters)\n",
    "km.fit(feature_matrix)\n",
    "\n",
    "# Create a dataframe out of the formed clusters\n",
    "clusters = km.labels_.tolist()\n",
    "docs = { 'id': df['id'], 'cluster': clusters }\n",
    "clusters_df = pd.DataFrame(docs, columns = ['id', 'cluster'])\n",
    "\n",
    "# Show a sample of the result of the clustering\n",
    "clusters_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the keywords/terms per cluster, we need to extract the k-means clustering object from the indices of terms that compose each cluster. In order to get the top defining terms, we reverse the ordering so we get the closest term to the cluster center as the first element in the list. These steps are accomplished in a single of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0\n",
      "related, contact, information, september, categories, navigation, partners, newsletter, description, service, products, upcoming, archives, judonotes, twitter, education, similar, coupons, coupon, welcome \n",
      "\n",
      "Cluster #1\n",
      "yachting, angling, telephone, galleries, fishing, private, competition, functions, sponsors, information, corporate, welcome, heaviest, weddings, history, inhouse, management, membership, monthly, committee \n",
      "\n",
      "Cluster #2\n",
      "easy, nameclassifier, udt, author, status, phase, class, proposed, advanced, notes, project, modified, connection, appears, version, type, direction, generalization, complexity, priority \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We proceed further to get only the top 20 terms per cluster\n",
    "cluster_terms = {k: [] for k in clusters}\n",
    "for i in range(nclusters):\n",
    "    cluster_top_terms = [feature_names[x] for x in order_centroids[i, :20]]\n",
    "    cluster_terms[i] = cluster_top_terms\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "clusters_df.reset_index(inplace=True)\n",
    "\n",
    "# We fuse the raw dataframe with the clusters dataframe so get\n",
    "# a mapping of the record ID, keywords, and cluster in one dataframe\n",
    "df.set_index('id', inplace=True)\n",
    "clusters_df.set_index('id', inplace=True)\n",
    "#combined_df = df.join(clusters_df, how='outer', on='id')\n",
    "combined_df = df.merge(clusters_df)\n",
    "\n",
    "# Then from the combined dataframe we can generate a good summary\n",
    "# of the clusters with cluster number, the count of records, and their defining terms\n",
    "cdf = combined_df.groupby('cluster').agg('count')\n",
    "cdf.rename(columns={'keywords': 'count'}, inplace=True)\n",
    "cdf['keywords'] = [', '.join(cluster_terms[x]) for x in cdf.index]\n",
    "\n",
    "for i,row in cdf.iterrows():\n",
    "    print('Cluster #%s' % row.name)\n",
    "    print(row.keywords, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have three clusters defined by a collection of characteristic keywords. The keywords associated with these clusters can be used to demonstrate record auto-classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Saving the Clustering Results\n",
    "\n",
    "For the auto-classification step, at least the generated clusters and the vectorizer object are needed. So we need to dumpt those into file. In Python, we do that using the `pickle` package. It serializes any complex Python object/data structure that can then be loaded later on for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "results = {\n",
    "    'vectorizer': vectorizer,\n",
    "    'features': feature_matrix,\n",
    "    'clusters': combined_df,\n",
    "    'terms': cluster_terms\n",
    "}\n",
    "\n",
    "if not os.path.exists('temp'):\n",
    "    os.mkdir('temp')\n",
    "    \n",
    "pickle.dump(results, open('temp/clusters.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above step (saving the cluster model to disk) is necessary for the next stage, building an auto-classifier based on keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
