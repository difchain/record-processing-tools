{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification\n",
    "\n",
    "This notebook demonstrates the automated classification of records into clusters identified in a prior auto-clustering step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Training Data\n",
    "\n",
    "We are going to use the results of the auto-clustering of the existing documents as training data to build our classifier model. First step then will be to load the results of the clustering.\n",
    "\n",
    "Loading the pickled clustering results will look for the `tokenize()` function defined in the clustering notebook. We will have restate the function definition here for the loading to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # get the english stopwords\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.wordpunct_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        # include only those that contains letters\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            # exclude stop words, those shorter than 3 characters, and those that\n",
    "            # start with non-alphanumeric characters\n",
    "            if token not in stopwords and len(token) > 2 and token[0].isalnum():\n",
    "                filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the training data (i.e. the results of the clustering)\n",
    "clustering_output= pickle.load(open('temp/clusters.pkl', 'rb'))\n",
    "clusters = clustering_output['clusters']\n",
    "vectorizer = clustering_output['vectorizer']\n",
    "feature_matrix = clustering_output['features']\n",
    "cluster_terms = clustering_output['terms']\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Classifier Model\n",
    "\n",
    "We will use a classifier that implements regularized linear models with stochastic gradient descent (SGD) learning.\n",
    "\n",
    "Read more here: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Instantiate the classifier object and define the options\n",
    "classifier = SGDClassifier(loss='hinge', penalty='l2', \n",
    "    alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "\n",
    "# Perform the fitting of the feature matrix to the clusters\n",
    "classifier.fit(feature_matrix, clusters['cluster'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be saved for reuse later, instead of building again from training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['temp/classifier.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# We dump both the vectorizer and the classifier into a pickle file\n",
    "outfile = 'temp/classifier.pkl'\n",
    "joblib.dump((vectorizer, classifier), outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying New Records\n",
    "\n",
    "Once a trained classifier model is built, it can be used to classify new records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_records = [\n",
    "    ('record_id_1', 'Testimonials, companies, strategy, FinTech, session, website, financial, Enterprise, results'),\n",
    "    ('record_id_2', 'Contact, Location, Biology, Accelerate, Learning, Machine, High-Throughput, Computational'),\n",
    "    ('record_id_3', 'Personalized, Recipe, completely, different, Nutrition, country, another, Welcome, recipes, Research'),\n",
    "    ('record_id_4', 'Contact, information, facilities, department, Categories'),\n",
    "    ('record_id 5', 'Program, programs, Contact, students, Graduate, requirements, Curriculum')\n",
    "]\n",
    "\n",
    "ndf = pd.DataFrame(new_records, columns=['id', 'keywords'])\n",
    "counts = vectorizer.transform(ndf['keywords'].values)\n",
    "predictions = classifier.predict(counts)\n",
    "\n",
    "# Show the predictions\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>record_id_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>record_id_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>record_id_3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>record_id_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>record_id 5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cluster\n",
       "0  record_id_1        1\n",
       "1  record_id_2        1\n",
       "2  record_id_3        1\n",
       "3  record_id_4        1\n",
       "4  record_id 5        1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's make a dataframe to have a nice display of results\n",
    "results = pd.DataFrame(list(zip(ndf['id'].values, predictions)), columns=['id', 'cluster'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validating the Model\n",
    "\n",
    "We can then proceed to cross-validate the model and see how accurate it is in reclassifying our training data into their respective clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records classified: 10000\n",
      "Accuracy score: 88.43%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Use the pipelining style to \n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  vectorizer),\n",
    "    ('classifier',  classifier)\n",
    "])\n",
    "\n",
    "# Split the training data to 3 partitions -- 2 parts for training, 1 part for testing\n",
    "# This will also generate the permutations so that all of the three parts can be used for testing\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "scores = []\n",
    "\n",
    "# Iterate over the permutations\n",
    "for train_indices, test_indices in kf.split(clusters):\n",
    "    \n",
    "    # Extract the keywords and clusters for the training data\n",
    "    train_text = clusters.iloc[train_indices]['keywords'].values\n",
    "    train_y = clusters.iloc[train_indices]['cluster'].values\n",
    "    \n",
    "    # Extract the keywords and clusters for the testing data\n",
    "    test_text = clusters.iloc[test_indices]['keywords'].values\n",
    "    test_y = clusters.iloc[test_indices]['cluster'].values\n",
    "    \n",
    "    # Use the pipeline interface to build the classifier model\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    \n",
    "    # Classify the records in the testing set\n",
    "    predictions = pipeline.predict(test_text)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    score = accuracy_score(test_y, predictions)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Total records classified:', len(clusters))\n",
    "print('Accuracy score: {0:.2f}%'.format(sum(scores)/len(scores) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
